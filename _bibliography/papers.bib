@misc{sun2022fewshot,
      title={Few-shot Text Classification with Dual Contrastive Consistency}, 
      author={Liwen Sun and Jiawei Han},
      year={2022},
      arxiv={2209.15069},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      pdf={https://arxiv.org/abs/2209.15069},
      publisher={arXiv},
      abbr={ArXiv},
      abstract={In this paper, we explore how to utilize pre-trained language model to perform few-shot text classification where only a few annotated examples are given for each class. Since using traditional cross-entropy loss to fine-tune language model under this scenario causes serious overfitting and leads to sub-optimal generalization of model, we adopt supervised contrastive learning on few labeled data and consistency-regularization on vast unlabeled data. Moreover, we propose a novel contrastive consistency to further boost model performance and refine sentence representation. After conducting extensive experiments on four datasets, we demonstrate that our model (FTCC) can outperform state-of-the-art methods and has better robustness.}
}